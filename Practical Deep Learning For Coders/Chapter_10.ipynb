{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/Machine-Learning-Notes/blob/main/Practical%20Deep%20Learning%20For%20Coders/Chapter_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Deep Dive: RNNs"
      ],
      "metadata": {
        "id": "OdW8RlBCOnCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The Basic Overview:**\n",
        "---"
      ],
      "metadata": {
        "id": "YuNQS1hFOyUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Chapter 1, the book introduced how deep learning can achieve great results with natural language datasets using pretrained language models fine-tuned for specific tasks. This chapter dives deeper into the foundational concepts and processes behind training language models, particularly for NLP tasks.\n"
      ],
      "metadata": {
        "id": "SZgVgSHb2bHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. **Language Modeling and Transfer Learning**\n",
        "---"
      ],
      "metadata": {
        "id": "DS8EHlHq2f_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Concepts:**\n",
        "- **Self-Supervised Learning**: Training a model using labels embedded in the data itself, such as predicting the next word in a sentence.\n",
        "  - This task forces the model to develop an understanding of the language.\n",
        "- **Universal Language Model Fine-tuning (ULMFiT)**: Introduced an additional step of fine-tuning a pretrained language model to the target corpus before fine-tuning it for classification tasks. This process improves predictions by:\n",
        "  1. Fine-tuning the language model on the specific corpus (e.g., IMDb movie reviews).\n",
        "  2. Using the fine-tuned model as the base for classification.\n",
        "---\n",
        "**Transfer Learning Stages in NLP:**\n",
        "1. Pretrain a language model on a large corpus (e.g., Wikipedia).\n",
        "2. Fine-tune the pretrained model to the target corpus.\n",
        "3. Fine-tune the model for the specific classification task.\n",
        "---"
      ],
      "metadata": {
        "id": "GyXMeaXn2x5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. **Building a Language Model**\n",
        "---"
      ],
      "metadata": {
        "id": "qz77RPr12-xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text Preprocessing Steps:***\n",
        "1. **Tokenization**:\n",
        "   - Splits text into smaller units (tokens).\n",
        "   - Methods include:\n",
        "     - **Word-based**: Splits based on spaces and rules (e.g., `don’t` -> `do n’t`).\n",
        "     - **Subword-based**: Splits words into common substrings (e.g., `occasion` -> `o c ca sion`).\n",
        "     - **Character-based**: Splits into individual characters.\n",
        "\n",
        "   Fastai provides `Tokenizer` to handle tokenization, adding special tokens like `xxbos` (start of text), `xxmaj` (capitalized word), and `xxunk` (unknown word).\n",
        "\n",
        "2. **Numericalization**:\n",
        "   - Converts tokens into numbers using a vocabulary.\n",
        "   - Fastai provides utilities to:\n",
        "     - Use existing pretrained vocabularies.\n",
        "     - Initialize embeddings for new words with random vectors.\n",
        "\n",
        "3. **Language Model Data Loader Creation**:\n",
        "   - `fastai.LMDataLoader` offsets independent and dependent variables by one token.\n",
        "   - Ensures proper shuffling while maintaining structure.\n",
        "\n",
        "4. **Language Model Creation**:\n",
        "   - Requires a model that can handle sequences of variable lengths (e.g., RNNs).\n",
        "---\n"
      ],
      "metadata": {
        "id": "lDxUS3AZ3Jtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. **Tokenization Using fastai**\n",
        "---"
      ],
      "metadata": {
        "id": "1y3QTgMO3wnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fastai uses external libraries like spaCy for tokenization.\n",
        "- Default rules include:\n",
        "  - `fix_html`: Replaces HTML characters.\n",
        "  - `replace_rep`: Handles repeated characters (e.g., `xxxx` -> `xxrep 4 x`).\n",
        "  - `replace_maj`: Handles capitalization.\n",
        "  - `lowercase`: Converts text to lowercase and adds `xxbos` and `xxeos` tokens.\n"
      ],
      "metadata": {
        "id": "5Un8QV-Y346u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Example:\n",
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)\n",
        "files = get_text_files(path, folders=['train', 'test', 'unsup'])\n",
        "txt = files[0].open().read()\n",
        "spacy = WordTokenizer()\n",
        "tkn = Tokenizer(spacy)\n",
        "toks = tkn(txt)\n",
        "print(coll_repr(toks, 30))"
      ],
      "metadata": {
        "id": "FYoKwpXl3913",
        "outputId": "ac325282-1061-4020-ffc8-ea9fca01290d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:10&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#303) ['xxbos','xxmaj','the','true','story','of','a','bunch','of','junkies','robbing','a','not','so','honest','businessman','of','drugs',',','jewelry',',','guns',',','and','money','.','xxmaj','some','would','say'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. **Subword Tokenization**\n",
        "---"
      ],
      "metadata": {
        "id": "NcPB3mtS4Ct4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Useful for languages without spaces (e.g., Chinese) or languages with long compound words (e.g., Turkish).\n",
        "- Steps:\n",
        "  1. Analyze a corpus to find common substrings.\n",
        "  2. Tokenize using these substrings."
      ],
      "metadata": {
        "id": "wmY0eBlH4KAQ"
      }
    }
  ]
}